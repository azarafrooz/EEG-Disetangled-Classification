{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disentangling time-variant and time-invariant factors for improved classification of EEG signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two main challenges in classifying stimuli using EEG signals are 1) Low signal-to-noise ratio. This is due to the time-variant factors appearing in the course of measurements. These time-varying factors can be electrical surroundings, muscle activity, eye movements of blinks, etc. 2) Variabilities between individual subjects.\n",
    "\n",
    "To this end, we propose a novel architecture based on the recent development of disentangled representation and probabilistic sequential modeling. The underlying architecture is a Conv1dLSTM, that utilizes **only the invariant factors** for classification. We hoped that disentangling time-varying and time-invariant dynamics apparent in the sequence of EEG data, increase the classification accuracy. Our experiment using MIIR dataset shows we can achieve accuracy of 21.67% in test time, verified using the outer 9-fold cross-validation performed across subjects as in [1](http://bib.sebastianstober.de/icassp2017.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mirr dataset contains 64 EEG channels, 9 subjects and 12 audio stimuli for 540 trails. Measurements sequences of are length 3518. They have been normalized to zero-mean and range[-1,1].\n",
    "Therefore no *normalization/zfiltering* is necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seq length of 3518 is way longer than the 250-300 steps used in practice for LSTM. We, therefore, first apply a Conv1d with a kernel size of 320 and stride 160 to reduce the length of sequences to 20. We used factored disentangled representation for sequential data, described in the paper [2](https://arxiv.org/pdf/1803.02991.pdf). Using similar techniques presented in [3](https://openreview.net/pdf?id=Sy2fzU9gl), [2](https://arxiv.org/pdf/1803.02991.pdf) derives time-variant encodings $\\mathcal{z}$ and time-invariant features $\\mathcal{f}$ for sequential data. Our architecture has 2 main differences. 1) First, we are concerned with classification rather than data generation. The decoder is, therefore, is replaced with a classifier and the reconstruction loss is replaced with CrossEntropy loss. 2) Most importantly, unlike [2](https://arxiv.org/pdf/1803.02991.pdf) where ($\\mathcal{z}$, $\\mathcal{f}$) is passed to the decoder, we only use $\\mathcal{f}$ to output classifications. This is to ignore time-variant factor/noises appearing in the course of experiments. \n",
    "\n",
    "Also, since small amount of trial data is available, we use batchsize = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation scheme \n",
    "Verification is being conducted using the outer 9-fold cross-validation performed across subjects as in [1](http://bib.sebastianstober.de/icassp2017.pdf).\n",
    "A random subject is excluded from the training and the rest of the data get used for the training. The data for the excluded subject then gets used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Average Loss: 3.520056616763274 KL of f : 0.07574303398529689 KL of z : 0.9584266122741004 Cross Entropy: 2.4858869741360348 Test Accuracy: 0.13333333333333333\n",
      "Epoch 21 : Average Loss: 1.8140081226825715 KL of f : 0.2328883560995261 KL of z : 0.005657989709288813 Cross Entropy: 1.5754617758095264 Test Accuracy: 0.15\n",
      "Epoch 41 : Average Loss: 1.30403537551562 KL of f : 0.20091642954697211 KL of z : 0.0011133064908790402 Cross Entropy: 1.1020056409140428 Test Accuracy: 0.2\n",
      "Epoch 61 : Average Loss: 1.1021551149586837 KL of f : 0.11758367288857699 KL of z : 0.0002490390420462063 Cross Entropy: 0.9843224037438632 Test Accuracy: 0.16666666666666666\n",
      "Epoch 81 : Average Loss: 1.0411095894873141 KL of f : 0.08546531063814958 KL of z : 5.902535857937134e-05 Cross Entropy: 0.9555852508793274 Test Accuracy: 0.2\n",
      "Epoch 101 : Average Loss: 0.999629090850552 KL of f : 0.059354947817822294 KL of z : 1.890480166556093e-05 Cross Entropy: 0.9402552363773187 Test Accuracy: 0.18333333333333332\n",
      "Epoch 121 : Average Loss: 0.9927772662291924 KL of f : 0.054650107646981874 KL of z : 1.1512146794245837e-05 Cross Entropy: 0.9381156470626593 Test Accuracy: 0.2\n",
      "Epoch 141 : Average Loss: 0.9735517871876558 KL of f : 0.04458397471656402 KL of z : 9.037441147796698e-06 Cross Entropy: 0.9289587754756212 Test Accuracy: 0.18333333333333332\n",
      "Epoch 161 : Average Loss: 0.9632283881306648 KL of f : 0.036102156589428586 KL of z : 6.997134655269596e-06 Cross Entropy: 0.9271192343284687 Test Accuracy: 0.16666666666666666\n",
      "Epoch 181 : Average Loss: 0.9631867634753386 KL of f : 0.03727598376572132 KL of z : 7.320030530166832e-06 Cross Entropy: 0.9259034615010023 Test Accuracy: 0.21666666666666667\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import *\n",
    "import h5py\n",
    "\n",
    "from model import *\n",
    "import config\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "class Miir(data.Dataset):\n",
    "    def __init__(self, data_path=config.DATA_PATH, train=True):\n",
    "        h5 = h5py.File(data_path, 'r')\n",
    "        self.train = train\n",
    "\n",
    "        features = h5['features']\n",
    "        targets = h5['targets']\n",
    "        subjects = h5['subjects']\n",
    "\n",
    "        self.test_subject_id = random.randint(0,8)\n",
    "        train_indxs = [i for i, e in enumerate(subjects) if e != self.test_subject_id]\n",
    "\n",
    "        self.train_features = [e for i, e in enumerate(features) if i in train_indxs]\n",
    "        self.test_features = [e for i, e in enumerate(features) if i not in train_indxs]\n",
    "\n",
    "        self.train_targets = [e for i, e in enumerate(targets) if i in train_indxs]\n",
    "        self.test_targets = [e for i, e in enumerate(targets) if i not in train_indxs]\n",
    "\n",
    "        self.train_subjects = [e for i, e in enumerate(subjects) if i in train_indxs]\n",
    "        self.test_subjects = [e for i, e in enumerate(subjects) if i not in train_indxs]\n",
    "\n",
    "        self.train_size = len(self.train_features)\n",
    "        self.test_size = len(self.test_features)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return self.train_size\n",
    "        else:\n",
    "            return self.test_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            return self.train_features[idx], self.train_targets[idx], self.train_subjects[idx]\n",
    "        else:\n",
    "            return self.test_features[idx], self.test_targets[idx], self.test_subjects[idx]\n",
    "\n",
    "\n",
    "def loss_fn(target, pred_target, f_mean, f_logvar, z_post_mean, z_post_logvar, z_prior_mean, z_prior_logvar):\n",
    "    \"\"\"\n",
    "    :param target:\n",
    "    :param pred_target\n",
    "    :param f_mean:\n",
    "    :param f_logvar:\n",
    "    :param z_post_mean:\n",
    "    :param z_post_logvar:\n",
    "    :param z_prior_mean:\n",
    "    :param z_prior_logvar:\n",
    "    :return:\n",
    "    Loss function consists of 3 parts, Cross Entropy of the predicted targes and the target, the KL divergence of f,\n",
    "    and the sum over the KL divergence of each z_t, with the sum divided by batch_size.\n",
    "    Loss = {CrossEntropy(pred_target, target) + KL of f + sum(KL of z_t)}/batch_size\n",
    "    Prior of f is a spherical zero_mean unit variance Gaussian and the prior for each z_t is a Gaussian whose\n",
    "    mean and variance are given by LSTM.\n",
    "    \"\"\"\n",
    "    batch_size = target.size(0)\n",
    "    cross_entropy = F.cross_entropy(pred_target, target)\n",
    "    kld_f = - 0.5 * torch.sum(1+f_logvar - torch.pow(f_mean,2) - torch.exp(f_logvar))\n",
    "    z_post_var = torch.exp(z_post_logvar)\n",
    "    z_prior_var = torch.exp(z_prior_logvar)\n",
    "    kld_z = 0.5 * torch.mean(z_prior_logvar - z_post_logvar + ((z_post_var + torch.pow(z_post_mean - z_prior_mean, 2))\n",
    "                                                               / z_prior_var) - 1)\n",
    "\n",
    "    return (cross_entropy + (kld_f + kld_z)) / batch_size, kld_f / batch_size, kld_z / batch_size,\\\n",
    "           cross_entropy/batch_size\n",
    "\n",
    "\n",
    "def save_model(model, optim, epoch, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch+1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'opimizer': optim.state_dict()}, path)\n",
    "\n",
    "\n",
    "def check_accuracy(model, test):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct_target = 0\n",
    "    with torch.no_grad():\n",
    "        for item in test:\n",
    "            features, target, subject = item\n",
    "            target = torch.argmax(target, dim=1) # one-hot back to int\n",
    "            *_, pred_target = model(features)\n",
    "            _, pred_target = torch.max(pred_target.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct_target+=(pred_target==target).sum().item()\n",
    "    model.train()\n",
    "    return correct_target/total\n",
    "\n",
    "\n",
    "def train_classifier(model, optim, dataset, epochs, path, test, start = 0):\n",
    "    model.train()\n",
    "    for epoch in range(start, epochs):\n",
    "        losses = []\n",
    "        kld_fs = []\n",
    "        kld_zs = []\n",
    "        cross_entropies = []\n",
    "        \n",
    "        for i, item in enumerate(dataset,1):\n",
    "            features, target, subject = item\n",
    "            target = torch.argmax(target, dim=1)  # one hot back to int\n",
    "            optim.zero_grad()\n",
    "            f_mean, f_logvar, f, z_post_mean, z_post_logvar, z, z_prior_mean,\\\n",
    "            z_prior_logvar, pred_target = model(features)\n",
    "            loss, kld_f, kld_z, cross_entropy = loss_fn(target, pred_target, f_mean, f_logvar,\n",
    "                                                       z_post_mean, z_post_logvar, z_prior_mean, z_prior_logvar)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            losses.append(loss.item())\n",
    "            kld_fs.append(kld_f.item())\n",
    "            kld_zs.append(kld_z.item())\n",
    "            cross_entropies.append(cross_entropy.item())\n",
    "\n",
    "        test_accuracy = check_accuracy(model, test)\n",
    "        meanloss = np.mean(losses)\n",
    "        meanf = np.mean(kld_fs)\n",
    "        meanz = np.mean(kld_zs)\n",
    "        mean_cross_entropies = np.mean(cross_entropies)\n",
    "        if epoch%20==0: #print out result every 20 epochs\n",
    "            print(\"Epoch {} : Average Loss: {} KL of f : {} KL of z : {} \"\n",
    "                  \"Cross Entropy: {} Test Accuracy: {}\".format(epoch + 1, meanloss, meanf, meanz, mean_cross_entropies,\n",
    "                                                               test_accuracy))\n",
    "        save_model(model, optim, epoch, path)\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    model = DisentangledEEG(factorized=True, nonlinearity=True)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "    train_data = Miir(config.DATA_PATH, True)\n",
    "    test_data = Miir(config.DATA_PATH, False)\n",
    "    loader = data.DataLoader(train_data, batch_size=1, shuffle=True, num_workers=1)\n",
    "    loader_test = data.DataLoader(test_data, batch_size=60, shuffle=True, num_workers=4)\n",
    "    train_classifier(model=model, optim=optim, dataset=loader, epochs=200,\n",
    "                     path='./checkpoint_disentangled_classifier.pth', test=loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
